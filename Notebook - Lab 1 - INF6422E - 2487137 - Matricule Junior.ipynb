{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acff94c3",
   "metadata": {},
   "source": [
    "# INF6422E – Advanced Concepts in Computer Security  \n",
    "## Practical Work 1 – Winter 2026  \n",
    "\n",
    "### Intrusion Detection System and Its Evaluation\n",
    "\n",
    "--- \n",
    "  \n",
    "## Students  \n",
    "- Antoine Khoueiry – *Matricule:* 2487137  \n",
    "- Louis – *Matricule:* XXXXXXX  \n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "The objective of this practical work is to design, evaluate, and compare classical machine learning–based Intrusion Detection Systems (IDS) using a standard cybersecurity dataset.  \n",
    "The focus is on quantitative performance evaluation, analysis of trade-offs between detection accuracy and false alarms, and discussion of real-world deployment constraints of IDS solutions.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset\n",
    "\n",
    "This study uses the **CICIDS2017** dataset, combining:\n",
    "- **Monday traffic** (Benign baseline)\n",
    "- **Wednesday traffic** (Benign + DoS + Heartbleed)\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "1. Dataset Analysis and Preprocessing  \n",
    "2. Classical Machine Learning Models for IDS  \n",
    "3. Ensemble-Based IDS and Performance Trade-offs \n",
    "4. Comparative Performance Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9782d",
   "metadata": {},
   "source": [
    "# 1. Dataset Analysis and Preprocessing\n",
    "\n",
    "## Dataset Construction (Monday + Wednesday)\n",
    "\n",
    "- **Monday-WorkingHours.pcap_ISCX**: contains only benign (normal) traffic and serves as a baseline.\n",
    "- **Wednesday-workingHours.pcap_ISCX**: contains a mixture of benign traffic and multiple attack types: DoS and Heartbleed.\n",
    "\n",
    "The following steps load the raw CSV files and merge them into a single dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57b916b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monday dataset shape: (529918, 79)\n",
      "Wednesday dataset shape: (692703, 79)\n",
      "Column structure verified: identical columns\n",
      "Merged dataset shape: (1222621, 79)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Paths to CICIDS2017 CSV files\n",
    "monday_path = \"MachineLearningCSV/MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv\"  \n",
    "wednesday_path = \"MachineLearningCSV/MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv\"\n",
    "\n",
    "# Load datasets\n",
    "df_monday = pd.read_csv(monday_path)\n",
    "df_wednesday = pd.read_csv(wednesday_path)\n",
    "\n",
    "print(\"Monday dataset shape:\", df_monday.shape)\n",
    "print(\"Wednesday dataset shape:\", df_wednesday.shape)\n",
    "\n",
    "\n",
    "# Ensure both datasets have identical columns\n",
    "assert list(df_monday.columns) == list(df_wednesday.columns), \\\n",
    "    \"Column mismatch between Monday and Wednesday datasets\"\n",
    "\n",
    "print(\"Column structure verified: identical columns\")\n",
    "\n",
    "\n",
    "# Merge datasets\n",
    "df = pd.concat([df_monday, df_wednesday], axis=0, ignore_index=True)\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "print(\"Merged dataset shape:\", df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dbc7ef",
   "metadata": {},
   "source": [
    "## 1.1 Dataset Exploration\n",
    "\n",
    "This section presents a statistical exploration of the constructed dataset combining Monday (benign baseline) and Wednesday (benign and attack traffic) from the CICIDS2017 dataset.\n",
    "\n",
    "The analysis focuses on:\n",
    "- the total number of samples,\n",
    "- the number of features,\n",
    "- the distribution of benign versus attack traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9605bd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 1222621\n",
      "Total number of features: 79\n"
     ]
    }
   ],
   "source": [
    "# Number of samples and features\n",
    "num_samples, num_features = df.shape\n",
    "\n",
    "print(\"Total number of samples:\", num_samples)\n",
    "print(\"Total number of features:\", num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e1679c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "BENIGN              969949\n",
      "DoS Hulk            231073\n",
      "DoS GoldenEye        10293\n",
      "DoS slowloris         5796\n",
      "DoS Slowhttptest      5499\n",
      "Heartbleed              11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Distribution of traffic labels\n",
    "label_distribution = df[\"Label\"].value_counts()\n",
    "\n",
    "print(label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b842cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryLabel\n",
      "Benign    969949\n",
      "Attack    252672\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Benign vs Attack distribution\n",
    "df[\"BinaryLabel\"] = df[\"Label\"].apply(lambda x: \"Benign\" if x == \"BENIGN\" else \"Attack\")\n",
    "\n",
    "binary_distribution = df[\"BinaryLabel\"].value_counts()\n",
    "print(binary_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd772099",
   "metadata": {},
   "source": [
    "### Statistical Summary and Relevance for IDS Evaluation\n",
    "\n",
    "The constructed dataset contains **1,222,621 network traffic samples** described by **79 features**, providing a high-dimensional representation of network behavior. The dataset combines a clean benign baseline on Monday with attack traffic collected on Wednesday.\n",
    "\n",
    "The traffic distribution reveals a strong class imbalance. Benign traffic represents **969,949 samples**, while attack traffic accounts for **252,672 samples**. Among malicious activities, **DoS attacks dominate**, particularly *DoS Hulk* (231,073 samples), while other attack types such as *GoldenEye*, *Slowloris*, *Slowhttptest*, and *Heartbleed* appear much less frequently, with Heartbleed being extremely rare (11 samples).\n",
    "\n",
    "These characteristics are highly relevant for Intrusion Detection System (IDS) evaluation. The class imbalance reflects real-world network conditions, where malicious traffic is significantly rarer than normal traffic. However, such imbalance can bias learning algorithms toward the benign class, potentially increasing false negatives. Additionally, the presence of multiple attack types with varying frequencies challenges IDS models to generalize across both high-volume and low-frequency attacks. As a result, evaluation metrics beyond accuracy, such as precision, recall, and false alarm rate, become critical for assessing IDS effectiveness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226bfb78",
   "metadata": {},
   "source": [
    "## 1.2 Dataset Preprocessing\n",
    "\n",
    "This section describes the preprocessing steps applied to the dataset prior to training machine learning models.\n",
    "\n",
    "The preprocessing pipeline includes:\n",
    "- cleaning,\n",
    "- encoding of categorical features,\n",
    "- feature standardization,\n",
    "- splitting the dataset into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd2696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape after cleaning: (1220887, 80)\n"
     ]
    }
   ],
   "source": [
    "'''Cleaning'''\n",
    "\n",
    "# Replace infinite values with NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(\"Dataset shape after cleaning:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce81827",
   "metadata": {},
   "source": [
    "Some features contained infinite values due to zero-duration flows. These values were replaced with NaN and the corresponding samples were removed to ensure numerical stability during model training.\n",
    "We also see the number of columns added 1. It's \"BinaryLabel\" column added at the end of question 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb5fddd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1220887, 78)\n",
      "Labels shape: (1220887,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and labels\n",
    "X = df.drop(columns=[\"Label\", \"BinaryLabel\"])\n",
    "y = df[\"BinaryLabel\"]\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Labels shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b674a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric feature columns: []\n",
      "Number of non-numeric feature columns: 0\n"
     ]
    }
   ],
   "source": [
    "'''Categorical Feature Encoding'''\n",
    "\n",
    "# Check if there are categorical (non-numeric) features\n",
    "non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"Non-numeric feature columns:\", non_numeric_cols)\n",
    "print(\"Number of non-numeric feature columns:\", len(non_numeric_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5effc0fa",
   "metadata": {},
   "source": [
    "The CICIDS2017 feature set is composed of numerical traffic statistics. We verified the data types of the input features and found no non-numeric (categorical) feature columns. Therefore, no categorical encoding (e.g., one-hot encoding) was required. The `Label` column is treated as the target variable and was handled separately from the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e56b700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Feature standardization'''\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8118aeb",
   "metadata": {},
   "source": [
    "Although Logistic Regression can operate with both normalization and standardization, standardization was chosen in this work as it provides better numerical stability and robustness to outliers, which are common in network traffic features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329b211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 854620\n",
      "Validation set size: 183133\n",
      "Test set size: 183134\n"
     ]
    }
   ],
   "source": [
    "'''Splitting the dataset into training, validation, and test sets'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train split (70%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_scaled, y,\n",
    "    test_size=0.30,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Validation (15%) and Test (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.50,\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Validation set size:\", X_val.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0118ef5f",
   "metadata": {},
   "source": [
    "### Split Strategy Justification\n",
    "\n",
    "The dataset is split into 70% training, 15% validation, and 15% test sets. The training set is used to learn model parameters, while the validation set supports model selection and hyperparameter tuning without biasing the final evaluation. The test set remains unseen during training and provides an unbiased estimate of real-world IDS performance.\n",
    "\n",
    "'*Stratified*' splitting is applied to preserve the original class distribution across all subsets. This is particularly important in intrusion detection contexts, where class imbalance is common and improper splitting could lead to misleading performance estimates or overfitting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
